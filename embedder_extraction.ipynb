{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder extraction tutorial\n",
    "This notebook will show you how to extract partition from a model, aka. `Transfer Learning` or `Fine-Tuning`. <br /> \n",
    "The souce instructions can be found [here](https://www.tensorflow.org/guide/keras/transfer_learning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps are like this:\n",
    "1. Setting up the [notebook's variables](#/Environment-setup)\n",
    "2. Loading the `model` from an `.h5` file\n",
    "3. Extracting the embedding layer from the model\n",
    "4. Added new input layer\n",
    "5. Testing the new model \n",
    "5. Save the model again as `.h5` file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final, Tuple, Any\n",
    "import os\n",
    "\n",
    "TUTORIAL_WORKSPACE: Final[str] = 'tutorial_workspace'\n",
    "MODEL_INPUT_PATH: Final[str] = os.path.join(TUTORIAL_WORKSPACE, 'outputs', 'siamesemodel.h5')\n",
    "EMBEDDING_LAYER_NAME: Final[str] = 'embedding'\n",
    "POSITIVE_FOLDER_PATH: Final[str] = os.path.join(TUTORIAL_WORKSPACE, 'positives')\n",
    "NEGATIVE_FOLDER_PATH: Final[str] = os.path.join(TUTORIAL_WORKSPACE, 'negatives')\n",
    "NEW_MODEL_PATH: Final[str] = os.path.join(TUTORIAL_WORKSPACE, 'outputs', 'embedding_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model successfully, we will need to create the custom classes that created alongside the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    # Smiliarity calculation\n",
    "    def call(self, input_embeddings, validation_embeddings) -> Any:\n",
    "        return tf.math.abs(input_embeddings - validation_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will load the model from it's `.h5` file and use it in our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Model reload from file\n",
    "model: Model = tf.keras.models.load_model(\n",
    "    MODEL_INPUT_PATH, \n",
    "    \n",
    "    # Adding the custom layers we created on our own\n",
    "    custom_objects={\n",
    "        'L1Dist': L1Dist, # Custom layer that need to be loaded\n",
    "        'BinaryCrossentropy': tf.losses.BinaryCrossentropy\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_image (InputLayer)  [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_image[0][0]',            \n",
      "                                                                  'validation_image[0][0]']       \n",
      "                                                                                                  \n",
      " l1_dist_2 (L1Dist)             (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['l1_dist_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the embedding layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the embedding layer from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer: Layer = model.get_layer(EMBEDDING_LAYER_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_SHAPE: Final[Tuple[int, int, int]] = (100, 100, 3)\n",
    "\n",
    "# Anchor image input in the network\n",
    "input_image = Input(name='input_image', shape=INPUT_IMAGE_SHAPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's connect them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "_embedding_layer = embedding_layer(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"face_embedder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Functional)      (None, 4096)              38960448  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = Model(inputs=[input_image], outputs=[_embedding_layer], name='face_embedder')\n",
    "new_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing that model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our new model for a ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.typing.types import EagerTensor\n",
    "\n",
    "def preprocess(file_path: str):\n",
    "    '''\n",
    "        This function will load with tensorflow this image\n",
    "        and resize it to the apropriate dimensions needed for our neural network.\n",
    "        \n",
    "        args:\n",
    "            file_path: str - path to the target image file\n",
    "\n",
    "        returns: \n",
    "            2d array of the pixels of the image \n",
    "    '''\n",
    "\n",
    "    # Read image bytes from file path\n",
    "    byte_image = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Loading the bytes as image\n",
    "    image = tf.io.decode_jpeg(byte_image)\n",
    "\n",
    "    # Resize the image to 100x100 pixels\n",
    "    image: EagerTensor = tf.image.resize(image, (100, 100))\n",
    "    \n",
    "    # Devide each pixel between (0 and 1) instead of (0 and 255)\n",
    "    image /= 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(p_folder_path: str, n_folder_path = None) -> Tuple[Tuple, Tuple]:\n",
    "    if n_folder_path == None:\n",
    "        all_files: list[str] = os.listdir(p_folder_path)\n",
    "\n",
    "        first_shot = preprocess(os.path.join(p_folder_path, all_files[0]))\n",
    "        second_shot = preprocess(os.path.join(p_folder_path, all_files[1]))\n",
    "\n",
    "        print(np.array([first_shot]).shape)\n",
    "\n",
    "        first_p_embeddings = new_model.predict(np.array([first_shot]))\n",
    "        print('first_p_embeddings:', first_p_embeddings.shape)\n",
    "\n",
    "        second_p_embeddings = new_model.predict(np.array([second_shot]))\n",
    "        print('second_p_embeddings:', second_p_embeddings.shape)\n",
    "\n",
    "    else:\n",
    "        all_positives_files: list[str] = os.listdir(p_folder_path)\n",
    "        all_negatives_files: list[str] = os.listdir(n_folder_path)\n",
    "\n",
    "        first_shot = preprocess(os.path.join(p_folder_path, all_positives_files[0]))\n",
    "        second_shot = preprocess(os.path.join(n_folder_path, all_negatives_files[0]))\n",
    "\n",
    "        print(np.array([first_shot]).shape)\n",
    "\n",
    "        first_p_embeddings = new_model.predict(np.array([first_shot]))\n",
    "        print('first_p_embeddings:', first_p_embeddings.shape)\n",
    "\n",
    "        second_p_embeddings = new_model.predict(np.array([second_shot]))\n",
    "        print('second_p_embeddings:', second_p_embeddings.shape)\n",
    "\n",
    "    return (first_p_embeddings, second_p_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distance between the vectors (remember, both of the embeddings vectors are the same person) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 100, 3)\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "first_p_embeddings: (1, 4096)\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "second_p_embeddings: (1, 4096)\n",
      "match_percentage: 0.925%\n"
     ]
    }
   ],
   "source": [
    "first_p_embeddings, second_p_embeddings = calculate_embeddings(POSITIVE_FOLDER_PATH)\n",
    "\n",
    "l1_val = tf.math.abs(first_p_embeddings - second_p_embeddings)\n",
    "match_percentage = 1 - np.average(l1_val)\n",
    "\n",
    "print('match_percentage: {:.3f}%'.format(match_percentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's chack a negative test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 100, 3)\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "first_p_embeddings: (1, 4096)\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "second_p_embeddings: (1, 4096)\n",
      "match_percentage: 0.576%\n"
     ]
    }
   ],
   "source": [
    "first_p_embeddings, second_p_embeddings = calculate_embeddings(POSITIVE_FOLDER_PATH, NEGATIVE_FOLDER_PATH)\n",
    "\n",
    "l1_val = tf.math.abs(first_p_embeddings - second_p_embeddings)\n",
    "match_percentage = 1 - np.average(l1_val)\n",
    "\n",
    "print('match_percentage: {:.3f}%'.format(match_percentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the modified model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "new_model.save(NEW_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb6876993b2b103376e027649eaab835bc81729f875a1d0f5f440f0228b14c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
