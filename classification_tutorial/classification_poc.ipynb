{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ab3ae10",
   "metadata": {},
   "source": [
    "# Classification POC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ed077a0",
   "metadata": {},
   "source": [
    "This notebook will describe the process of face recognition using `elasticsearch` and `deepface` package."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "147d1315",
   "metadata": {},
   "source": [
    "## Roadamp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6809ee8",
   "metadata": {},
   "source": [
    "1. Face Detection\n",
    "2. Embeddings generation\n",
    "3. Data Saving\n",
    "4. Passport Query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67884872",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab64423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Final\n",
    "\n",
    "WORKDIR_PATH: Final[str] = 'classification_poc'\n",
    "PASSPORTS_FOLDER_PATH: Final[str] = os.path.join(WORKDIR_PATH, 'passports')\n",
    "TEST_IMAGES_FOLDER_PATH: Final[str] = os.path.join(WORKDIR_PATH, 'test_images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d88d46e2",
   "metadata": {},
   "source": [
    "## 1. Face Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8722f602",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0687c501-eaa4-42aa-b467-c8e1df6a3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "210e283e",
   "metadata": {},
   "source": [
    "### Detecting faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbfbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "        This function detectes the faces of a given image\n",
    "        Args:\n",
    "            image_path: str - Path to the image on the disk\n",
    "        Returns:\n",
    "            List of the faces detected in the given image alongside some metadata\n",
    "    \"\"\"\n",
    "    assert os.path.exists(image_path), 'Can not detect faces of image that does not exist'\n",
    "\n",
    "    backends: list = [\n",
    "      'opencv', \n",
    "      'ssd', \n",
    "      'dlib', \n",
    "      'mtcnn', \n",
    "      'retinaface', \n",
    "      'mediapipe'\n",
    "    ]\n",
    "\n",
    "    face_locations = DeepFace.extract_faces(image_path, detector_backend=backends[4])\n",
    "    return face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_files: list[str] = os.listdir(TEST_IMAGES_FOLDER_PATH)\n",
    "all_faces_detected: list[list[dict]] = []\n",
    "\n",
    "for filename in tqdm(all_files):\n",
    "    file_path = os.path.join(TEST_IMAGES_FOLDER_PATH, filename)\n",
    "    try:\n",
    "        faces_in_image = detect_faces(file_path)\n",
    "        all_faces_detected.append({ \n",
    "            'filename': file_path,\n",
    "            'detection_result': faces_in_image\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print('Failed to detect faces at:', e, file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b7b72fa",
   "metadata": {},
   "source": [
    "Let's check how much files we have processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705638b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All files processed:', len(all_faces_detected))\n",
    "\n",
    "faces_sum = 0\n",
    "for faces in all_faces_detected:\n",
    "    print(f'Found {len(faces[\"detection_result\"])} faces detected at image: {faces[\"filename\"]}')\n",
    "    faces_sum += len(faces[\"detection_result\"])\n",
    "\n",
    "print('Total faces detected:', faces_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2803113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def mark_image(image: np.ndarray, target_positions: list[dict]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Adding layer of rectangles of by target positions\n",
    "        Args:\n",
    "            image: np.ndarray - Source image to add the layer on\n",
    "            target_positions: list[dict] - List of dictionaries that discribes the positions of the wanted targets\n",
    "        Returns:\n",
    "            The modified image\n",
    "    \"\"\"\n",
    "\n",
    "    modified_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    for target_position in target_positions:\n",
    "        # Adding mark at the found target\n",
    "        modified_image = cv2.rectangle(\n",
    "            modified_image, \n",
    "            (target_position['x'], target_position['y']), \n",
    "            (target_position['x'] + target_position['w'], target_position['y'] + target_position['h']), \n",
    "            (255,0,0), 10\n",
    "        )\n",
    "\n",
    "    return modified_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efa1dd1c",
   "metadata": {},
   "source": [
    "Now let's make sure that the found faces are accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change this line as you want\n",
    "FILE_INDEX: Final[int] = -5\n",
    "\n",
    "file_to_show = all_faces_detected[FILE_INDEX]\n",
    "all_positions: list = [ face['facial_area'] for face in file_to_show[\"detection_result\"] ]\n",
    "m_image = mark_image(cv2.imread(file_to_show['filename']), all_positions)\n",
    "plt.imshow(m_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d663abc7",
   "metadata": {},
   "source": [
    "## 2. Embeddings Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2038a45",
   "metadata": {},
   "source": [
    "### Crop the positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8508edca",
   "metadata": {},
   "source": [
    "Let's crop the image by the coordinates we got"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9ed1067",
   "metadata": {},
   "source": [
    "Now we got the faces positions at the image, Let's crop it and prepare it to the next phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e5f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['face', 'facial_area', 'confidence'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_sample: dict = all_faces_detected[0]\n",
    "detection_result: dict = face_sample['detection_result'][0]\n",
    "detection_result.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23e3b1e8",
   "metadata": {},
   "source": [
    "### Embeddings generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89215720",
   "metadata": {},
   "source": [
    "After we have the crops of each face in bits (variable, without saving them to the disk). <br/> \n",
    "Let's create the embeddings of that face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b5849436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embeddings(cropped_image: np.array) -> list[dict]:\n",
    "    \"\"\"\n",
    "        This function generates embeddings for an image bits\n",
    "        Args:\n",
    "            cropped_image: np.array - Bits of an image \n",
    "        Returns:\n",
    "            List of all the `embeddings`, `facial_area` at the given image (per face)\n",
    "    \"\"\"\n",
    "    models: Final[list[str]] = [\n",
    "        \"VGG-Face\", # [input-layer: (224, 224)] -> (2622d) \n",
    "        \"Facenet\", # Google's -> (128d) ELASTIC_SUPPORT\n",
    "        \"Facenet512\", # Google's -> (512d) ELASTIC_SUPPORT\n",
    "        \"OpenFace\", # [input-layer: (96, 96, 3)] -> (128d) ELASTIC_SUPPORT\n",
    "        \"DeepFace\", # Meta's [input-layer: (152, 152, 3)] -> (4096d)\n",
    "        \"DeepID\", # [input-layer: (47, 55, 3)] -> (160d) ELASTIC_SUPPORT\n",
    "        \"ArcFace\", # [input-layer: (112, 112, 3)] -> (512d) ELASTIC_SUPPORT\n",
    "        \"Dlib\", # [input-layer: (150, 150, 3)] -> (128d) ELASTIC_SUPPORT\n",
    "        \"SFace\" # [input-layer: (160, 160, 3)] -> (128d) ELASTIC_SUPPORT\n",
    "    ]\n",
    "\n",
    "    return DeepFace.represent(\n",
    "        img_path = cropped_image,\n",
    "        model_name = models[2],\n",
    "        detector_backend='skip' # Skipping face-detection step\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fc0ceb2",
   "metadata": {},
   "source": [
    "Little example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43659d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def crop_image(image_path: str, x: int, y: int, width: int, height: int, custom_shape: Tuple[int, int] = None) -> np.array:\n",
    "    \"\"\"\n",
    "        This function crops the image by coordinate and width, height\n",
    "        Args:\n",
    "            image_path: str - Image path to crop\n",
    "            x: int - X position to start the crop from # TODO: Check whether it's the top left or bottom right corner\n",
    "            y: int - Y position to start the crop from # TODO: Check whether it's the top left or bottom right corner\n",
    "            width: int - Width of the crop image\n",
    "            height: int - Height of the crop image\n",
    "            custom_shape: Tuple[int] = None - In case you want to modify the shape of the output crop\n",
    "        Returns:\n",
    "            Numpy array containing the crop image bits\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    crop_img: np.array = img[y:y+height, x:x+width]\n",
    "\n",
    "    if custom_shape:\n",
    "        crop_img = cv2.resize(crop_img, custom_shape)\n",
    "\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf87c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data: dict = face_sample['detection_result'][0]['facial_area']\n",
    "cropped_image: np.array = crop_image(\n",
    "    image_path = face_sample['filename'], \n",
    "    x = crop_data['x'], \n",
    "    y = crop_data['y'], \n",
    "    width = crop_data['w'], \n",
    "    height = crop_data['h'],\n",
    "    # custom_shape=(160, 160) # Based on the embedding model [facenet512]\n",
    "    # custom_shape = ()\n",
    ")\n",
    "\n",
    "print(cropped_image.shape)\n",
    "cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "ax1.set_title('Full image')\n",
    "ax1.imshow(mark_image(cv2.imread(face_sample['filename']), [crop_data]))\n",
    "ax2.set_title('Face crop')\n",
    "ax2.imshow(cropped_image)\n",
    "\n",
    "cropped_image = np.asarray([cropped_image])\n",
    "print('cropped_image shape:', cropped_image.shape)\n",
    "\n",
    "embeddings = gen_embeddings(cropped_image)\n",
    "print('embeddings len:', len(embeddings[0]['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c394a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(cropped_image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Preprocess a cropped image for embeddings generation\n",
    "        Args:\n",
    "            cropped_image: np.ndarray - The cropped image bits\n",
    "        \n",
    "        Returns:\n",
    "            Returns the new cropped image as an `np.ndarray`\n",
    "    \"\"\"\n",
    "\n",
    "    _cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR)\n",
    "    return np.asarray([_cropped_image])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da140075",
   "metadata": {},
   "source": [
    "Now, after we found the full sized image and the crops and have generated the embeddings for each crop. <br/>\n",
    "Let's write a function that does that logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "62df2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:08<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_embeddings: list[list] = []\n",
    "\n",
    "# all_faces_detected it's actually a list of all the files\n",
    "for file_description in tqdm(all_faces_detected):\n",
    "    faces: list[dict] = file_description['detection_result'] # All the faces detected in that file\n",
    "    for i in tqdm(range(len(faces)), leave=False):\n",
    "        face = faces[i]\n",
    "        crop_data: dict = face['facial_area']\n",
    "        cropped_image = crop_image(\n",
    "            image_path = file_description['filename'], \n",
    "            x = crop_data['x'], \n",
    "            y = crop_data['y'], \n",
    "            width = crop_data['w'], \n",
    "            height = crop_data['h'],\n",
    "            custom_shape=(160, 160) # Based on the embedding model\n",
    "        )\n",
    "        \n",
    "        cropped_image = preprocess(cropped_image)\n",
    "\n",
    "        # Because we already have cropped the images to one face in an image\n",
    "        # So we can assume that always we will have just one embedding in a result\n",
    "        embeddings: list[dict] = gen_embeddings(cropped_image)\n",
    "        all_embeddings.append({\n",
    "            'face_embeddings': embeddings[0]['embedding'],\n",
    "            'filename': file_description['filename'],\n",
    "            'face_position': face['facial_area']\n",
    "        })\n",
    "\n",
    "print(len(all_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f091ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "426b85d0",
   "metadata": {},
   "source": [
    "## 3. Data Saving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9ad84fc",
   "metadata": {},
   "source": [
    "Before we start indexing the data into the databse, we need to connect to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86f78989",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTIC_PROTOCOL: Final[str] = 'http://'\n",
    "ELASTIC_PORT: Final[str] = '30000'\n",
    "ELASTIC_HOSTS: Final[str] = ['10.0.0.13']\n",
    "INDEX_NAME: Final[str] = 'tests_vectors_test_index'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7678ebe",
   "metadata": {},
   "source": [
    "### Database initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eb087aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch([f'{ELASTIC_PROTOCOL}{host}:{ELASTIC_PORT}' for host in ELASTIC_HOSTS])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "534db9ea",
   "metadata": {},
   "source": [
    "Don't forget to check the connection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb68eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_exists: bool = es.indices.exists(index=INDEX_NAME)\n",
    "is_exists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0db92d78",
   "metadata": {},
   "source": [
    "In case you want to reset this index, run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fc28a2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index=INDEX_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d8db1ad",
   "metadata": {},
   "source": [
    "### Index mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ff7fbea",
   "metadata": {},
   "source": [
    "Let's create new index with a mapping to tell the database the embeddings field should be indexed as a vector and not regular list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b0e5ff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'tests_vectors_test_index'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {\n",
    "    'properties': {\n",
    "        'face_embeddings': {\n",
    "            'type': 'dense_vector',\n",
    "            'dims': 512, # Critical field, that should be precise\n",
    "            'index': True\n",
    "        },\n",
    "        'filename': {\n",
    "            'type': 'keyword'\n",
    "        },\n",
    "        'face_position': {\n",
    "            'type': 'object'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=INDEX_NAME, mappings=mapping, ignore=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c1a855",
   "metadata": {},
   "source": [
    "### Actual data saving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9033edbe",
   "metadata": {},
   "source": [
    "So we have a database connection, we also have the data. <br/>\n",
    "Let's put the data inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "60cfd66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:01<00:00, 112.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all documents inserted successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for face_document in tqdm(all_embeddings):\n",
    "    es.index(index=INDEX_NAME, document=face_document)\n",
    "\n",
    "print('all documents inserted successfully')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15bca426",
   "metadata": {},
   "source": [
    "## 4. Passport Image Query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a8dd444",
   "metadata": {},
   "source": [
    "Congratulations! <br/>\n",
    "\n",
    "We managed to detect all the faces in the given images <br/>\n",
    "Generate an embeddings vector for each face <br/>\n",
    "And index it into our database.\n",
    "\n",
    "Now it's our time to test that solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3d8b9fa",
   "metadata": {},
   "source": [
    "Let's pick an image to search by. \n",
    "For the simplicity of this test it's recommended to use an image with just one face in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7a53b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avilable files to select (images): ['kobi.jpg', 'amir.jpg', 'tal.JPG', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "print('Avilable files to select (images):', os.listdir(PASSPORTS_FOLDER_PATH))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35841808",
   "metadata": {},
   "source": [
    "Now we will detect the face in that picture (assuming the image has only one face)\n",
    "\n",
    "TODO: (Show plot of the detected faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8120234a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces detected in paspport image: 1\n"
     ]
    }
   ],
   "source": [
    "SELECTED_FILE: Final[str] = 'kobi.jpg' # Change this as you wish \n",
    "\n",
    "\n",
    "SELECTED_FILE_PATH: Final[str] = os.path.join(PASSPORTS_FOLDER_PATH, SELECTED_FILE)\n",
    "detected_face: list = detect_faces(SELECTED_FILE_PATH)\n",
    "print('Faces detected in paspport image:', len(detected_face))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63d97fc0",
   "metadata": {},
   "source": [
    "Let's extract the necessary bits from the full picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95598ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_data: dict = detected_face[0]['facial_area']\n",
    "cropped_image: np.ndarray = crop_image(\n",
    "    image_path = SELECTED_FILE_PATH,\n",
    "    x = cropping_data['x'],\n",
    "    y = cropping_data['y'],\n",
    "    width = cropping_data['w'],\n",
    "    height = cropping_data['h'],\n",
    "    custom_shape = (224, 224)\n",
    ")\n",
    "\n",
    "cropped_image: np.ndarray = preprocess(cropped_image)\n",
    "plt.imshow(cropped_image[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88143c67",
   "metadata": {},
   "source": [
    "After we found a face, embeddings generation is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "02184b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding vector length: 512\n"
     ]
    }
   ],
   "source": [
    "embeddings = gen_embeddings(cropped_image)\n",
    "vector_to_search: np.ndarray = embeddings[0]['embedding']\n",
    "\n",
    "print('embedding vector length:', len(vector_to_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.search(index=INDEX_NAME, size=100, query={\n",
    "    \"script_score\": {\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"script\": {\n",
    "            \"source\": \"cosineSimilarity(params.query_vector, 'face_embeddings')\",\n",
    "            \"params\": {\n",
    "                \"query_vector\": vector_to_search\n",
    "            }\n",
    "        }\n",
    "    }    \n",
    "})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = res['hits']['hits'][2]['_source']\n",
    "src_image = cv2.imread(hit['filename'])\n",
    "target_position = hit['face_position']\n",
    "\n",
    "np.savetxt('hit.em', hit['face_embeddings'])\n",
    "\n",
    "print(hit['filename'])\n",
    "modified_image = mark_image(src_image, [target_position])\n",
    "plt.imshow(modified_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7bb6876993b2b103376e027649eaab835bc81729f875a1d0f5f440f0228b14c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
