{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot face recognition tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial based on this [video](https://www.youtube.com/watch?v=LKispFFQ5GU) will use tensorflow v2 to create a face recognition model based on the one-shot architecture. <br/>\n",
    "\n",
    "The steps are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import all the shared libraries that notebook uses section-wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Noam\\one-shot-fr-tutorial\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "from typing import Final, Any\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a workspace with all the necessary folders in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary folders created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "WORKSPACE_DIR_PATH: Final[str] = 'tutorial_workspace'\n",
    "DOWNLOADS_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'downloads')\n",
    "ANCHORS_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'anchors')\n",
    "POSITIVES_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'positives')\n",
    "NEGATIVES_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'negatives')\n",
    "\n",
    "os.makedirs(DOWNLOADS_PATH, exist_ok=True)\n",
    "os.makedirs(ANCHORS_PATH, exist_ok=True)\n",
    "os.makedirs(POSITIVES_PATH, exist_ok=True)\n",
    "os.makedirs(NEGATIVES_PATH, exist_ok=True)\n",
    "\n",
    "print('All necessary folders created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model a dataset is necessary. <br/>\n",
    "For that we will use the `LFW` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use named `LFW` and you can find it either in this [link](http://vis-www.cs.umass.edu/lfw/#download) or at the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import requests\n",
    "\n",
    "TAR_URL: Final[str] = 'http://vis-www.cs.umass.edu/lfw/lfw.tgz'\n",
    "FILE_NAME: Final[str] = 'lfw.tgz'\n",
    "file_path: str = path.join(DOWNLOADS_PATH, FILE_NAME)\n",
    "\n",
    "if not path.exists(file_path):\n",
    "    # Make an HTTP request within a context manager\n",
    "    with requests.get(TAR_URL, stream=True) as r:\n",
    "        \n",
    "        # Check header to get content length, in bytes\n",
    "        total_length = int(r.headers.get(\"Content-Length\"))\n",
    "        \n",
    "        # Implement progress bar via tqdm\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=total_length, desc=\"lfw.tar compressed dataset\") as raw:\n",
    "        \n",
    "            # Save the output to a file\n",
    "            with open(file_path, 'wb')as output:\n",
    "                shutil.copyfileobj(raw, output)\n",
    "else:\n",
    "    print('Dataset already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, the model needs a lot faces that not match to the anchors. <br/>\n",
    "For that we will use the dataset we downloaded above.\n",
    "\n",
    "So, we will extract the compressed files. and put them into the `NEGATIVES_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13233/13233 [01:22<00:00, 159.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from typing import IO, Iterable\n",
    "\n",
    "file_path: str = path.join(DOWNLOADS_PATH, 'lfw.tgz')\n",
    "\n",
    "with tarfile.open(file_path, 'r:gz') as compressed_file:\n",
    "    # Filtering out all directories and non-jpg files\n",
    "    all_members: Iterable[tarfile.TarInfo] = [m for m in compressed_file.getmembers() if '.jpg' in m.name or '.png' in m.name]\n",
    "    \n",
    "    for member in tqdm(iterable=all_members, total=len(all_members)):\n",
    "        archive_filename: str = member.path.split('/')[-1]\n",
    "        filename: str = path.join(NEGATIVES_PATH, archive_filename)\n",
    "        \n",
    "        buffer_reader: IO[bytes] = compressed_file.extractfile(member)\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(buffer_reader.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect positive and anchors data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we successfully loaded the `LFW` dataset into our workspace as negative shots (observations).\n",
    "\n",
    "Now, let's dive into the positives.\n",
    "So, we can get those observations in many ways. Like taking shots from the camera, use existing images. <br/>\n",
    "Let's do both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking shots from webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we take shots from the camera, we need to remember. The dataset has 250x250 pixels images. <br/>\n",
    "For the simplicity of this project, we will preffer to use the exact same size with the new shots.\n",
    "\n",
    "So remember, 250x250..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to import necessary packages for that phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is connected\n"
     ]
    }
   ],
   "source": [
    "# In case it not working properly, you can try with different index\n",
    "cap = cv2.VideoCapture(0)\n",
    "is_device_connected: bool = cap.isOpened()\n",
    "print(f\"The device is { 'connected' if is_device_connected else 'disconnected' }\")\n",
    "while is_device_connected:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Cut a 250x250 pixels crop from the original feed\n",
    "    start_x: int = 200\n",
    "    start_y: int = 120\n",
    "    height: int = start_y + 250\n",
    "    width: int = start_x + 250\n",
    "    frame = frame[start_x:width, start_y:height, :]\n",
    "\n",
    "    # Collect anchor image\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Creating unique filename\n",
    "        generated_name: str = f'{uuid.uuid1()}.jpg'\n",
    "        img_name: str = path.join(ANCHORS_PATH, generated_name)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "\n",
    "    # Collect positive image\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Creating unique filename\n",
    "        generated_name: str = f'{uuid.uuid1()}.jpg'\n",
    "        img_name: str = path.join(POSITIVES_PATH, generated_name)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "\n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Frame shape:', frame.shape)\n",
    "plt.title('Last frame detected')\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In case the code above not working"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't know which `device id` to use in the opencv's video `device id`. <br />\n",
    "Let's search for all the devices available to take images from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing index 0\n",
      "Port 0 is working and reads images (480.0 x 640.0)\n",
      "Testing index 1\n",
      "Port 1 is not working.\n",
      "Testing index 2\n",
      "Port 2 is not working.\n",
      "Testing index 3\n",
      "Port 3 is not working.\n",
      "Testing index 4\n",
      "Port 4 is not working.\n",
      "Testing index 5\n",
      "Port 5 is not working.\n",
      "Testing index 6\n",
      "Port 6 is not working.\n",
      "([], [0], [1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "def list_ports():\n",
    "    \"\"\"\n",
    "        Test the ports and returns a tuple with the available ports and the ones that are working.\n",
    "    \"\"\"\n",
    "    non_working_ports = []\n",
    "    dev_port = 0\n",
    "    working_ports = []\n",
    "    available_ports = []\n",
    "    while len(non_working_ports) < 6: # if there are more than 5 non working ports stop the testing. \n",
    "        print(f'Testing index {dev_port}')\n",
    "        camera = cv2.VideoCapture(dev_port)\n",
    "        if not camera.isOpened():\n",
    "            non_working_ports.append(dev_port)\n",
    "            print(\"Port %s is not working.\" %dev_port)\n",
    "        else:\n",
    "            is_reading, img = camera.read()\n",
    "            w = camera.get(3)\n",
    "            h = camera.get(4)\n",
    "            if is_reading:\n",
    "                print(\"Port %s is working and reads images (%s x %s)\" %(dev_port,h,w))\n",
    "                working_ports.append(dev_port)\n",
    "            else:\n",
    "                print(\"Port %s for camera ( %s x %s) is present but does not reads.\" %(dev_port,h,w))\n",
    "                available_ports.append(dev_port)\n",
    "        dev_port +=1\n",
    "    return available_ports,working_ports,non_working_ports\n",
    "\n",
    "print(list_ports())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from existing images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternitivly, you can load the positive and anchors images from existing images files. <br/>\n",
    "Just copy your images positive images into `$POSITIVES_PATH`, and the anchors will be copied into `ANCHORS_PATH`.\n",
    "\n",
    "Don't forget to make sure that the photos you are importing have to be at shape (resolution) of `250px*250px`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data successfully collected. Let's start to prepare it to the training phase.\n",
    "\n",
    "So, what we will do in this phase is:\n",
    "* Create labeled dataset from the anchors, positives and negatives directories\n",
    "* Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import tensorflow as tf\n",
    "from tf_agents.typing.types import EagerTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that all of the images we will going to use, having the same 'shape' (dimensions). <br/>\n",
    "In case they are different, we will scale them to the apropriate dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x1d673ea9660>,\n",
       " <tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x1d673ea8eb0>,\n",
       " <tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x1d673eaaa70>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SIZE: Final[int] = 300\n",
    "\n",
    "# Tensorflow will take all files matching to the pattern inside `list_files()`\n",
    "anchor: tf.data.Dataset = tf.data.Dataset.list_files(f'{ANCHORS_PATH}/*.jpg').take(DATASET_SIZE)\n",
    "positive: tf.data.Dataset = tf.data.Dataset.list_files(f'{POSITIVES_PATH}/*.jpg').take(DATASET_SIZE)\n",
    "negative: tf.data.Dataset = tf.data.Dataset.list_files(f'{NEGATIVES_PATH}/*.jpg').take(DATASET_SIZE)\n",
    "\n",
    "negative.as_numpy_iterator(), anchor.as_numpy_iterator(), positive.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: This how you can iterate over a tensorlow Dataset class (in this case we will run over the anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'tutorial_workspace\\\\anchors\\\\de5caf61-9cb0-11ed-90af-b808cf4c5169.jpg'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path: str):\n",
    "    # Read image bytes from file path\n",
    "    byte_image = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Loading the bytes as image\n",
    "    image = tf.io.decode_jpeg(byte_image)\n",
    "\n",
    "    # Resize the image to 100x100 pixels\n",
    "    image: EagerTensor = tf.image.resize(image, (100, 100))\n",
    "    \n",
    "    # Devide each pixel between (0 and 1) instead of (0 and 255)\n",
    "    image /= 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be sure, let's verify that the pre process have done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate one of the lists \n",
    "_dup_anchor = anchor.as_numpy_iterator()\n",
    "\n",
    "# Running the preprocess on the current image\n",
    "img = preprocess(_dup_anchor.next())\n",
    "\n",
    "print('Minimum value in tensor (should be 0):', img.numpy().min())\n",
    "print('Max value in tensor (should be 1):', img.numpy().max())\n",
    "print('Image shape (should be (100, 100, 3)):', img.numpy().shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we will need to give it:\n",
    "* Positive observations (which means anchor image, positive image and result of 1).\n",
    "* Negative observations (which means anchor image, negative and result of 0).\n",
    "\n",
    "For that we will create two datasets, one for positive and one for negative. And concatenate them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor))) # Create a vector with shape of positive images\n",
    "negative_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor))) # Create a vector with shape of negative images\n",
    "\n",
    "# Telling the data loader to load each of those images with the proper label simultaneously \n",
    "positive_dataset = tf.data.Dataset.zip((anchor, positive, positive_labels))\n",
    "negative_dataset = tf.data.Dataset.zip((anchor, negative, negative_labels))\n",
    "\n",
    "# Concatenating the positive and negative datasets into a single dataset\n",
    "dataset = positive_dataset.concatenate(negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Little example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset.as_numpy_iterator()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'tutorial_workspace\\\\anchors\\\\d9f50c46-9cb0-11ed-925b-b808cf4c5169.jpg',\n",
       " b'tutorial_workspace\\\\positives\\\\d99051b8-9cb0-11ed-a54b-b808cf4c5169.jpg',\n",
       " 1.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train and test dataset partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all the images from it's path, and put them the proper label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img: str, validation_img: str, label: int) -> tuple[Any, Any, int]:\n",
    "    return (preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[0.57254905, 0.60784316, 0.6039216 ],\n",
       "         [0.5742647 , 0.6095588 , 0.60563725],\n",
       "         [0.5735294 , 0.60882354, 0.60490197],\n",
       "         ...,\n",
       "         [0.61960787, 0.6627451 , 0.64705884],\n",
       "         [0.622549  , 0.66568625, 0.65      ],\n",
       "         [0.622549  , 0.66568625, 0.65      ]],\n",
       " \n",
       "        [[0.5732843 , 0.60857844, 0.6046569 ],\n",
       "         [0.56789213, 0.60318625, 0.5992647 ],\n",
       "         [0.56960785, 0.60784316, 0.6039216 ],\n",
       "         ...,\n",
       "         [0.6193628 , 0.6634804 , 0.6477941 ],\n",
       "         [0.6186274 , 0.6627451 , 0.64705884],\n",
       "         [0.6186274 , 0.6627451 , 0.64705884]],\n",
       " \n",
       "        [[0.5688726 , 0.6041667 , 0.6002451 ],\n",
       "         [0.5627451 , 0.6009804 , 0.59705883],\n",
       "         [0.5688726 , 0.60784316, 0.6039216 ],\n",
       "         ...,\n",
       "         [0.6159314 , 0.6629902 , 0.64730394],\n",
       "         [0.61960787, 0.6666667 , 0.6509804 ],\n",
       "         [0.6166667 , 0.6637255 , 0.6480392 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.0875    , 0.07965686, 0.03259804],\n",
       "         [0.08651961, 0.07769608, 0.03357843],\n",
       "         [0.0872549 , 0.0754902 , 0.04019608],\n",
       "         ...,\n",
       "         [0.14289215, 0.14583333, 0.09289216],\n",
       "         [0.14313726, 0.14607844, 0.09313726],\n",
       "         [0.12352941, 0.12745099, 0.07254902]],\n",
       " \n",
       "        [[0.09044117, 0.08259804, 0.03553922],\n",
       "         [0.08651961, 0.07769608, 0.03357843],\n",
       "         [0.08137255, 0.06960785, 0.03431373],\n",
       "         ...,\n",
       "         [0.15196079, 0.15196079, 0.10490196],\n",
       "         [0.14485294, 0.14485294, 0.09779412],\n",
       "         [0.10073529, 0.10171568, 0.05269608]],\n",
       " \n",
       "        [[0.08529412, 0.07745098, 0.03039216],\n",
       "         [0.08529412, 0.07647059, 0.03235294],\n",
       "         [0.0754902 , 0.06372549, 0.02843137],\n",
       "         ...,\n",
       "         [0.15490197, 0.15490197, 0.10784314],\n",
       "         [0.12254902, 0.12254902, 0.0754902 ],\n",
       "         [0.07352941, 0.07352941, 0.02647059]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[0.5683824 , 0.60759807, 0.6036765 ],\n",
       "         [0.56691176, 0.60612744, 0.6022059 ],\n",
       "         [0.56691176, 0.60906863, 0.59632355],\n",
       "         ...,\n",
       "         [0.62058824, 0.6637255 , 0.6480392 ],\n",
       "         [0.62352943, 0.6666667 , 0.6509804 ],\n",
       "         [0.62352943, 0.6666667 , 0.6509804 ]],\n",
       " \n",
       "        [[0.56764704, 0.6068627 , 0.60294116],\n",
       "         [0.5629902 , 0.6022059 , 0.5982843 ],\n",
       "         [0.5615196 , 0.6007353 , 0.59681374],\n",
       "         ...,\n",
       "         [0.6117647 , 0.65882355, 0.64509803],\n",
       "         [0.6156863 , 0.6627451 , 0.6490196 ],\n",
       "         [0.61960787, 0.6666667 , 0.65294117]],\n",
       " \n",
       "        [[0.5615196 , 0.6007353 , 0.59681374],\n",
       "         [0.55759805, 0.59681374, 0.59289217],\n",
       "         [0.5637255 , 0.60294116, 0.5990196 ],\n",
       "         ...,\n",
       "         [0.6156863 , 0.6627451 , 0.654902  ],\n",
       "         [0.6164216 , 0.6634804 , 0.65563726],\n",
       "         [0.6156863 , 0.6627451 , 0.654902  ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.10588235, 0.09411765, 0.05882353],\n",
       "         [0.11372549, 0.10196079, 0.06666667],\n",
       "         [0.11764706, 0.10588235, 0.07058824],\n",
       "         ...,\n",
       "         [0.12671569, 0.12181372, 0.06838235],\n",
       "         [0.11862745, 0.11372549, 0.05441177],\n",
       "         [0.11078431, 0.10294118, 0.04411765]],\n",
       " \n",
       "        [[0.1127451 , 0.10098039, 0.06568628],\n",
       "         [0.11299019, 0.10122549, 0.06593137],\n",
       "         [0.11666667, 0.10490196, 0.06960785],\n",
       "         ...,\n",
       "         [0.09289216, 0.09387255, 0.04485294],\n",
       "         [0.09730392, 0.10122549, 0.04632353],\n",
       "         [0.10882353, 0.10980392, 0.05392157]],\n",
       " \n",
       "        [[0.10882353, 0.09705883, 0.06176471],\n",
       "         [0.11544117, 0.10367647, 0.06838235],\n",
       "         [0.11764706, 0.10588235, 0.07058824],\n",
       "         ...,\n",
       "         [0.06764706, 0.06764706, 0.02647059],\n",
       "         [0.08406863, 0.0870098 , 0.03406863],\n",
       "         [0.10220588, 0.10612745, 0.05122549]]], dtype=float32)>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = sample.next()\n",
    "res = preprocess_twin(*example)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after we have tested everything. Let's put all the things together. And build our dataloader pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset element_spec=(TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess_twin) # Running all over the dataset with the `preprocess_twin` function\n",
    "dataset = dataset.cache() # Caching the images\n",
    "dataset = dataset.shuffle(buffer_size=1024) # Shuffling the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = dataset.as_numpy_iterator().next()\n",
    "\n",
    "print('Label is:', samp[2])\n",
    "plt.imshow(samp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(samp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training and test dataset partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared variables for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_BATCH_SIZE: Final[int] = 16\n",
    "LEARNING_PREFETCH_SIZE: Final[int] = 8\n",
    "\n",
    "TRAIN_PARTITION_SIZE: Final[int] = round(len(dataset) * .7) # Get 70% of the dataset for training\n",
    "TEST_PARTITION_SIZE: Final[int] = round(len(dataset) * .3) # Get 30% of the dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training partition setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data: tf.data.Dataset = dataset.take(TRAIN_PARTITION_SIZE)\n",
    "training_data: tf.data.Dataset = training_data.batch(LEARNING_BATCH_SIZE) # Set batch size of 16\n",
    "training_data: tf.data.Dataset = training_data.prefetch(LEARNING_PREFETCH_SIZE)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup testing partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data: tf.data.Dataset = dataset.skip(TRAIN_PARTITION_SIZE) # Skipping of all the training partition data\n",
    "test_data: tf.data.Dataset = test_data.take(TEST_PARTITION_SIZE)\n",
    "test_data: tf.data.Dataset = test_data.batch(LEARNING_BATCH_SIZE)\n",
    "test_data: tf.data.Dataset = test_data.prefetch(LEARNING_PREFETCH_SIZE)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared imports for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# 100x100 px, with 3 colors channels (RGB)\n",
    "INPUT_IMAGE_SHAPE: Final[Tuple] = (100, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU memory limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you have a GPU installed on your machine. It's recommended to set a memory limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding() -> Model:\n",
    "    # Creating the input layer\n",
    "    input_layer = Input(shape=INPUT_IMAGE_SHAPE, name='input_image')\n",
    "\n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10, 10), activation='relu')(input_layer)\n",
    "    m1 = MaxPooling2D(64, (2, 2), padding='same')(c1)\n",
    "\n",
    "    # Second bloc\n",
    "    c2 = Conv2D(128, (7, 7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2, 2), padding='same')(c2)\n",
    "\n",
    "    # Third block\n",
    "    c3 = Conv2D(128, (4, 4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2, 2), padding='same')(c3)\n",
    "\n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "\n",
    "    return Model(inputs=[input_layer], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's try to compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 46, 46, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 40, 40, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 20, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 17, 17, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the embedding model/layer and showing the summary of it\n",
    "embedding: Model = make_embedding()\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Distance Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Need to check if arcface will perform better than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    # Smiliarity calculation\n",
    "    def call(self, input_embeddings, validation_embeddings) -> Any:\n",
    "        return tf.math.abs(input_embeddings - validation_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model() -> Model:\n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_image', shape=INPUT_IMAGE_SHAPE)\n",
    "\n",
    "    # Validation image in the network\n",
    "    validation_image = Input(name='validation_image', shape=INPUT_IMAGE_SHAPE)\n",
    "\n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    # Classification layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation of this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_image (InputLayer)  [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_image[0][0]',            \n",
      "                                                                  'validation_image[0][0]']       \n",
      "                                                                                                  \n",
      " distance (L1Dist)              (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            4097        ['distance[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the embedding model/layer and showing the summary of it\n",
    "siamese_model: Model = make_siamese_model()\n",
    "siamese_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps we will follow:\n",
    "1. Setup a loss function\n",
    "2. Setup an optimizer\n",
    "3. Build a Custom training step\n",
    "4. Create a Training loop\n",
    "5. Train the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb6876993b2b103376e027649eaab835bc81729f875a1d0f5f440f0228b14c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
