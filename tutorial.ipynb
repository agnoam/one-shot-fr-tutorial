{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot face recognition tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will use tensorflow v2 to create a face recognition model based on the one-shot architecture. <br/>\n",
    "\n",
    "The steps are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import all the shared libraries that notebook uses section-wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Final\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a workspace with all the necessary folders in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary folders created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "WORKSPACE_DIR_PATH: Final[str] = 'tutorial_workspace'\n",
    "DOWNLOADS_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'downloads')\n",
    "ANCHORS_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'anchors')\n",
    "POSITIVES_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'positives')\n",
    "NEGATIVES_PATH: Final[str] = path.join(WORKSPACE_DIR_PATH, 'negatives')\n",
    "\n",
    "os.makedirs(DOWNLOADS_PATH, exist_ok=True)\n",
    "os.makedirs(ANCHORS_PATH, exist_ok=True)\n",
    "os.makedirs(POSITIVES_PATH, exist_ok=True)\n",
    "os.makedirs(NEGATIVES_PATH, exist_ok=True)\n",
    "\n",
    "print('All necessary folders created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model a dataset is necessary. <br/>\n",
    "For that we will use the `LFW` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use named `LFW` and you can find it either in this [link](http://vis-www.cs.umass.edu/lfw/#download) or at the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import requests\n",
    "\n",
    "TAR_URL: Final[str] = 'http://vis-www.cs.umass.edu/lfw/lfw.tgz'\n",
    "FILE_NAME: Final[str] = 'lfw.tgz'\n",
    "file_path: str = path.join(DOWNLOADS_PATH, FILE_NAME)\n",
    "\n",
    "if not path.exists(file_path):\n",
    "    # Make an HTTP request within a context manager\n",
    "    with requests.get(TAR_URL, stream=True) as r:\n",
    "        \n",
    "        # Check header to get content length, in bytes\n",
    "        total_length = int(r.headers.get(\"Content-Length\"))\n",
    "        \n",
    "        # Implement progress bar via tqdm\n",
    "        with tqdm.wrapattr(r.raw, \"read\", total=total_length, desc=\"lfw.tar compressed dataset\") as raw:\n",
    "        \n",
    "            # Save the output to a file\n",
    "            with open(file_path, 'wb')as output:\n",
    "                shutil.copyfileobj(raw, output)\n",
    "else:\n",
    "    print('Dataset already downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, the model needs a lot faces that not match to the anchors. <br/>\n",
    "For that we will use the dataset we downloaded above.\n",
    "\n",
    "So, we will extract the compressed files. and put them into the `NEGATIVES_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18983/18983 [02:11<00:00, 144.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from typing import IO, Iterable\n",
    "\n",
    "file_path: str = path.join(DOWNLOADS_PATH, 'lfw.tgz')\n",
    "\n",
    "with tarfile.open(file_path, 'r:gz') as compressed_file:\n",
    "    # Filtering out all directories and non-jpg files\n",
    "    all_members: Iterable[tarfile.TarInfo] = [m for m in compressed_file.getmembers() if '.jpg' not in m.name and m.name != None]\n",
    "    \n",
    "    for member in tqdm(iterable=all_members, total=len(all_members)):\n",
    "        archive_filename: str = member.path.split('/')[-1]\n",
    "        filename: str = path.join(NEGATIVES_PATH, archive_filename)\n",
    "        \n",
    "        buffer_reader: IO[bytes] = compressed_file.extractfile(member)\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(buffer_reader.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect positive and anchors data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we successfully loaded the `LFW` dataset into our workspace as negative shots (observations).\n",
    "\n",
    "Now, let's dive into the positives.\n",
    "So, we can get those observations in many ways. Like taking shots from the camera, use existing images. <br/>\n",
    "Let's do both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking shots from webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we take shots from the camera, we need to remember. The dataset has 250x250 pixels images. <br/>\n",
    "For the simplicity of this project, we will preffer to use the exact same size with the new shots.\n",
    "\n",
    "So remember, 250x250..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import uuid\n",
    "\n",
    "# In case it not working properly, you can try with different index\n",
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Cut a 250x250 pixels crop from the original feed\n",
    "    start_x: int = 200\n",
    "    start_y: int = 120\n",
    "    height: int = start_y + 250\n",
    "    width: int = start_x + 250\n",
    "    frame = frame[start_x:width, start_y:height, :]\n",
    "\n",
    "    # Collect anchor image\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Creating unique filename\n",
    "        generated_name: str = f'{uuid.uuid1()}.jpg'\n",
    "        img_name: str = path.join(ANCHORS_PATH, generated_name)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "\n",
    "    # Collect positive image\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Creating unique filename\n",
    "        generated_name: str = f'{uuid.uuid1()}.jpg'\n",
    "        img_name: str = path.join(POSITIVES_PATH, generated_name)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "\n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Frame shape:', frame.shape)\n",
    "plt.title('Last frame detected')\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from existing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternitivly, you can load the positive and anchors images from existing images files. <br/>\n",
    "Just copy your images positive images into `$POSITIVES_PATH`, and the anchors will be copied into `ANCHORS_PATH`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data successfully collected. Let's start to prepare it to the training phase.\n",
    "\n",
    "So, what we will do in this phase is:\n",
    "* Create labeled dataset from the anchors, positives and negatives directories\n",
    "* Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import tensorflow as tf\n",
    "from tf_agents.typing.types import EagerTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that all of the images we will going to use, having the same 'shape' (dimensions). <br/>\n",
    "In case they are different, we will scale them to the apropriate dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x2be8d738f10>,\n",
       " <tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x2be8d7b48e0>,\n",
       " <tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x2be8d7b5270>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SIZE: Final[int] = 300\n",
    "\n",
    "# Tensorflow will take all files matching to the pattern inside `list_files()`\n",
    "anchor: tf.data.Dataset = tf.data.Dataset.list_files(f'{ANCHORS_PATH}/*.jpg').take(DATASET_SIZE)\n",
    "positive: tf.data.Dataset = tf.data.Dataset.list_files(f'{POSITIVES_PATH}/*.jpg').take(DATASET_SIZE)\n",
    "negative: tf.data.Dataset = tf.data.Dataset.list_files(f'{NEGATIVES_PATH}/*.jpg').take(DATASET_SIZE)\n",
    "\n",
    "negative.as_numpy_iterator(), anchor.as_numpy_iterator(), positive.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: This how you can iterate over a tensorlow Dataset class (in this case we will run over the anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'tutorial_workspace\\\\anchors\\\\94e4272f-3853-11ed-b580-c85b76d2a6ae.jpg'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path: str):\n",
    "    # Read image bytes from file path\n",
    "    byte_image = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Loading the bytes as image\n",
    "    image = tf.io.decode_jpeg(byte_image)\n",
    "\n",
    "    # Resize the image to 100x100 pixels\n",
    "    image: EagerTensor = tf.image.resize(image, (100, 100))\n",
    "    \n",
    "    # Devide each pixel between (0 and 1) instead of (0 and 255)\n",
    "    image /= 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be sure, let's verify that the pre process have done successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate one of the lists \n",
    "_dup_anchor = anchor.as_numpy_iterator()\n",
    "# Running the preprocess on the current image\n",
    "img = preprocess(_dup_anchor.next())\n",
    "\n",
    "print('Minimum value in tensor (should be 0):', img.numpy().min())\n",
    "print('Max value in tensor (should be 1):', img.numpy().max())\n",
    "print('Image shape (should be (100, 100, 3)):', img.numpy().shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we will need to give it:\n",
    "* Positive observations (which means anchor image, positive image and result of 1).\n",
    "* Negative observations (which means anchor image, negative and result of 0).\n",
    "\n",
    "For that we will create two datasets, one for positive and one for negative. And concatenate them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor))) # Create a vector with shape of positive images\n",
    "negative_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor))) # Create a vector with shape of negative images\n",
    "\n",
    "# Telling the data loader to load each of those images with the proper label simultaneously \n",
    "positive_dataset = tf.data.Dataset.zip((anchor, positive, positive_labels))\n",
    "negative_dataset = tf.data.Dataset.zip((anchor, negative, negative_labels))\n",
    "\n",
    "# Concatenating the positive and negative datasets into a single dataset\n",
    "dataset = positive_dataset.concatenate(negative_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Little example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset.as_numpy_iterator()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'tutorial_workspace\\\\anchors\\\\b2f63770-3853-11ed-b7c9-c85b76d2a6ae.jpg',\n",
       " b'tutorial_workspace\\\\positives\\\\6e10a761-3853-11ed-a140-c85b76d2a6ae.jpg',\n",
       " 1.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train and test dataset partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all the images from it's path, and put them the proper label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img: str, validation_img: str, label: int):\n",
    "    return (preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[0.75686276, 0.7764706 , 0.7607843 ],\n",
       "         [0.7607843 , 0.7794118 , 0.76666665],\n",
       "         [0.7607843 , 0.7764706 , 0.77254903],\n",
       "         ...,\n",
       "         [0.66887254, 0.71593136, 0.7080882 ],\n",
       "         [0.67156863, 0.7127451 , 0.7078431 ],\n",
       "         [0.6754902 , 0.7147059 , 0.7107843 ]],\n",
       " \n",
       "        [[0.75392157, 0.77254903, 0.75980395],\n",
       "         [0.75465685, 0.7703431 , 0.76642156],\n",
       "         [0.75784314, 0.7735294 , 0.7710784 ],\n",
       "         ...,\n",
       "         [0.6693627 , 0.7144608 , 0.70759803],\n",
       "         [0.6723039 , 0.7129902 , 0.7083333 ],\n",
       "         [0.67132354, 0.7115196 , 0.70465684]],\n",
       " \n",
       "        [[0.75686276, 0.77254903, 0.7745098 ],\n",
       "         [0.76004905, 0.7757353 , 0.7776961 ],\n",
       "         [0.7607843 , 0.7764706 , 0.779902  ],\n",
       "         ...,\n",
       "         [0.6723039 , 0.7115196 , 0.70759803],\n",
       "         [0.67156863, 0.7129902 , 0.702451  ],\n",
       "         [0.67156863, 0.7147059 , 0.6990196 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.96911764, 0.9963235 , 0.4122549 ],\n",
       "         [0.96544117, 0.9794118 , 0.3781863 ],\n",
       "         [0.94730395, 0.9458333 , 0.33578432],\n",
       "         ...,\n",
       "         [0.7943627 , 0.8492647 , 0.2735294 ],\n",
       "         [0.8754902 , 0.9372549 , 0.32107842],\n",
       "         [0.8865196 , 0.9463235 , 0.3360294 ]],\n",
       " \n",
       "        [[0.9588235 , 0.9411765 , 0.33431372],\n",
       "         [0.9227941 , 0.9102941 , 0.30784315],\n",
       "         [0.88014704, 0.86985296, 0.28995097],\n",
       "         ...,\n",
       "         [0.7026961 , 0.7495098 , 0.21642157],\n",
       "         [0.78431374, 0.8328431 , 0.26421568],\n",
       "         [0.8490196 , 0.8987745 , 0.30808824]],\n",
       " \n",
       "        [[0.9264706 , 0.90294117, 0.3245098 ],\n",
       "         [0.89705884, 0.87647057, 0.32107842],\n",
       "         [0.8392157 , 0.825     , 0.29166666],\n",
       "         ...,\n",
       "         [0.6911765 , 0.7372549 , 0.25490198],\n",
       "         [0.70637256, 0.7495098 , 0.25490198],\n",
       "         [0.7531863 , 0.7916667 , 0.2720588 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[0.79044116, 0.8061274 , 0.810049  ],\n",
       "         [0.7911765 , 0.8068628 , 0.81078434],\n",
       "         [0.78210783, 0.8036765 , 0.80465686],\n",
       "         ...,\n",
       "         [0.70980394, 0.74509805, 0.7411765 ],\n",
       "         [0.7058824 , 0.7411765 , 0.7372549 ],\n",
       "         [0.7058824 , 0.7411765 , 0.7372549 ]],\n",
       " \n",
       "        [[0.7921569 , 0.80784315, 0.8117647 ],\n",
       "         [0.7875    , 0.8066176 , 0.8068628 ],\n",
       "         [0.7823529 , 0.80588233, 0.7995098 ],\n",
       "         ...,\n",
       "         [0.70490193, 0.7441176 , 0.74019605],\n",
       "         [0.70416665, 0.7411765 , 0.7372549 ],\n",
       "         [0.70686275, 0.74215686, 0.7382353 ]],\n",
       " \n",
       "        [[0.7862745 , 0.80784315, 0.8029412 ],\n",
       "         [0.7882353 , 0.8117647 , 0.8039216 ],\n",
       "         [0.7882353 , 0.8117647 , 0.8039216 ],\n",
       "         ...,\n",
       "         [0.7019608 , 0.7411765 , 0.7372549 ],\n",
       "         [0.7019608 , 0.7411765 , 0.7372549 ],\n",
       "         [0.70980394, 0.74509805, 0.7411765 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.96887255, 0.96789217, 0.360049  ],\n",
       "         [0.96862745, 0.9602941 , 0.3622549 ],\n",
       "         [0.98284316, 0.96960783, 0.3644608 ],\n",
       "         ...,\n",
       "         [0.61470586, 0.6509804 , 0.1384804 ],\n",
       "         [0.6644608 , 0.69411767, 0.18357843],\n",
       "         [0.7169118 , 0.74828434, 0.21887255]],\n",
       " \n",
       "        [[0.8512255 , 0.8492647 , 0.2757353 ],\n",
       "         [0.8769608 , 0.86715686, 0.2887255 ],\n",
       "         [0.90710783, 0.89215684, 0.30588236],\n",
       "         ...,\n",
       "         [0.62058824, 0.6480392 , 0.1392157 ],\n",
       "         [0.6629902 , 0.6904412 , 0.18161765],\n",
       "         [0.7137255 , 0.74215686, 0.22156863]],\n",
       " \n",
       "        [[0.7262255 , 0.7291667 , 0.19338235],\n",
       "         [0.7740196 , 0.7713235 , 0.22598039],\n",
       "         [0.9504902 , 0.9463235 , 0.39215687],\n",
       "         ...,\n",
       "         [0.62058824, 0.6480392 , 0.1392157 ],\n",
       "         [0.6598039 , 0.6872549 , 0.17843138],\n",
       "         [0.7110294 , 0.73946077, 0.21887255]]], dtype=float32)>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = sample.next()\n",
    "res = preprocess_twin(*example)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after we have tested everything. Let's put all the things together. And build our dataloader pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset element_spec=(TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess_twin) # Running all over the dataset with the `preprocess_twin` function\n",
    "dataset = dataset.cache() # Caching the images\n",
    "dataset = dataset.shuffle(buffer_size=1024) # Shuffling the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = dataset.as_numpy_iterator().next()\n",
    "\n",
    "print('Label is:', samp[2])\n",
    "plt.imshow(samp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(samp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training and test dataset partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared variables for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_BATCH_SIZE: Final[int] = 16\n",
    "LEARNING_PREFETCH_SIZE: Final[int] = 8\n",
    "\n",
    "TRAIN_PARTITION_SIZE: Final[int] = round(len(dataset) * .7) # Get 70% of the dataset for training\n",
    "TEST_PARTITION_SIZE: Final[int] = round(len(dataset) * .3) # Get 30% of the dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training partition setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data: tf.data.Dataset = dataset.take(TRAIN_PARTITION_SIZE)\n",
    "training_data: tf.data.Dataset = training_data.batch(LEARNING_BATCH_SIZE) # Set batch size of 16\n",
    "training_data: tf.data.Dataset = training_data.prefetch(LEARNING_PREFETCH_SIZE)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup testing partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = dataset.skip(TRAIN_PARTITION_SIZE) # Skipping of all the training partition data\n",
    "test_data = test_data.take(TEST_PARTITION_SIZE)\n",
    "test_data = test_data.batch(LEARNING_BATCH_SIZE)\n",
    "test_data = test_data.prefetch(LEARNING_PREFETCH_SIZE)\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a93bc067222f0d000790a9b07c7609ede5d1d32935d4a0ad8e011a9603c9c828"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
